from data.as_dataframe import X_test, X_train, y_test, y_train

from models.linearSVC_iter1 import classifier
print(classifier.best_estimator_.named_steps['linear_svr'].coef_)
# GRE_SCORE -> 0.72
# TOEFL_SCORE -> 0.29
# CGPA -> 1.09
# RESEARCH -> 0.03
# REPUTATION -> 0.12

print(classifier.best_estimator_.named_steps['linear_svr'].coef_)
# GRE_SCORE -> 0.72
# TOEFL_SCORE -> 0.29
# CGPA -> 1.09
# RESEARCH -> 0.03
# REPUTATION -> 0.12

# WORST PREDICTIONS
train_predictions = classifier.predict(X_train)

prediction_errors = list(train_predictions - y_train)
absolute_predictions_errors = list(map(abs, prediction_errors))
training_copy = X_train.copy()
training_copy['GRE_SCORE'] = training_copy['GRE_SCORE'] / 340
training_copy['TOEFL_SCORE'] = training_copy['TOEFL_SCORE'] / 120
training_copy['CGPA'] = training_copy['CGPA'] / 10
training_copy['CHANCE_OF_ADMIT'] = y_train
training_copy['PREDICTION_ERRORS'] = prediction_errors
training_copy['ABS_PREDICTION_ERRORS'] = absolute_predictions_errors

print('20 LARGEST ABS_PREDICTION_ERRORS')
print(training_copy.nlargest(20, 'ABS_PREDICTION_ERRORS'))
#      GRE_SCORE  TOEFL_SCORE   CGPA  RESEARCH  REPUTATION  CHANCE_OF_ADMIT  PREDICTION_ERRORS  ABS_PREDICTION_ERRORS
# 92    0.876471     0.816667  0.803         0    0.600000             0.34           0.244018               0.244018
# 65    0.955882     0.933333  0.892         0    0.733333             0.55           0.237525               0.237525
# 64    0.955882     0.925000  0.870         0    0.633333             0.52           0.229544               0.229544
# 10    0.955882     0.883333  0.840         1    0.700000             0.52           0.218237               0.218237
# 59    0.914706     0.866667  0.830         0    0.400000             0.42           0.212377               0.212377
# 40    0.905882     0.916667  0.800         1    0.633333             0.46           0.200873               0.200873
# 94    0.891176     0.825000  0.766         0    0.500000             0.36           0.185109               0.185109
# 91    0.879412     0.808333  0.766         0    0.766667             0.38           0.182619               0.182619
# 66    0.961765     0.950000  0.902         0    0.600000             0.61           0.182111               0.182111
# 41    0.929412     0.875000  0.820         1    0.466667             0.49           0.178058               0.178058
# 61    0.902941     0.841667  0.820         0    0.666667             0.47           0.166546               0.166546
# 67    0.929412     0.891667  0.864         1    0.600000             0.57           0.166312               0.166312
# 42    0.920588     0.891667  0.850         1    0.433333             0.53           0.165465               0.165465
# 375   0.894118     0.841667  0.766         0    0.433333             0.38           0.164394               0.164394
# 95    0.894118     0.833333  0.784         0    0.533333             0.42           0.153131               0.153131
# 376   0.873529     0.800000  0.743         0    0.433333             0.34           0.152363               0.152363
# 413   0.932353     0.841667  0.794         1    0.533333             0.49           0.149764               0.149764
# 80    0.917647     0.875000  0.802         1    0.533333             0.50           0.147714               0.147714
# 327   0.867647     0.841667  0.786         0    0.433333             0.69 - 0.142773               0.142773
# 359   0.944118     0.891667  0.844         0    0.366667             0.81 - 0.137816               0.137816

print('20 SMALLEST ABS_PREDICTION_ERRORS')
print(training_copy.nsmallest(20, 'ABS_PREDICTION_ERRORS'))
#     GRE_SCORE  TOEFL_SCORE   CGPA  RESEARCH  REPUTATION  CHANCE_OF_ADMIT  PREDICTION_ERRORS  ABS_PREDICTION_ERRORS
# 472   0.961765     0.966667  0.948         1    0.833333             0.90      -9.992007e-16           9.992007e-16
# 365   0.970588     0.950000  0.917         1    0.766667             0.86      -5.673811e-05           5.673811e-05
# 349   0.920588     0.841667  0.804         0    0.566667             0.62       1.918615e-04           1.918615e-04
# 20    0.917647     0.891667  0.790         1    0.533333             0.64      -4.817476e-04           4.817476e-04
# 98    0.976471     0.991667  0.924         1    0.900000             0.90      -5.906712e-04           5.906712e-04
# 344   0.867647     0.800000  0.734         0    0.366667             0.47       6.323242e-04           6.323242e-04
# 397   0.970588     0.966667  0.945         1    0.900000             0.91       7.583264e-04           7.583264e-04
# 246   0.929412     0.875000  0.873         0    0.633333             0.72      -8.038818e-04           8.038818e-04
# 71    0.988235     0.933333  0.976         1    1.000000             0.96      -1.016385e-03           1.016385e-03
# 362   0.994118     0.958333  0.923         1    0.966667             0.91      -1.097642e-03           1.097642e-03
# 392   0.958824     0.933333  0.912         1    0.766667             0.84       1.179867e-03           1.179867e-03
# 328   0.952941     0.933333  0.877         1    0.766667             0.80      -1.184212e-03           1.184212e-03
# 113   0.941176     0.916667  0.856         0    0.633333             0.72       1.305672e-03           1.305672e-03
# 130   0.997059     0.950000  0.976         1    0.900000             0.96      -1.369249e-03           1.369249e-03
# 144   0.955882     0.933333  0.896         1    0.566667             0.80      -1.485085e-03           1.485085e-03
# 32    0.994118     0.983333  0.940         1    0.766667             0.91       1.639760e-03           1.639760e-03
# 439   0.917647     0.875000  0.846         0    0.433333             0.66      -1.780885e-03           1.780885e-03
# 53    0.952941     0.933333  0.810         1    0.700000             0.72      -1.918663e-03           1.918663e-03
# 201   0.926471     0.916667  0.846         1    0.566667             0.72      -1.942066e-03           1.942066e-03
# 462   0.902941     0.875000  0.794         0    0.666667             0.62      -2.026021e-03           2.026021e-03
# 472   0.961765     0.966667  0.948         1    0.833333             0.90 - 9.992007e-16           9.992007e-16
# 365   0.970588     0.950000  0.917         1    0.766667             0.86 - 5.673811e-05           5.673811e-05
# 349   0.920588     0.841667  0.804         0    0.566667             0.62       1.918615e-04           1.918615e-04
# 20    0.917647     0.891667  0.790         1    0.533333             0.64 - 4.817476e-04           4.817476e-04
# 98    0.976471     0.991667  0.924         1    0.900000             0.90 - 5.906712e-04           5.906712e-04
# 344   0.867647     0.800000  0.734         0    0.366667             0.47       6.323242e-04           6.323242e-04
# 397   0.970588     0.966667  0.945         1    0.900000             0.91       7.583264e-04           7.583264e-04
# 246   0.929412     0.875000  0.873         0    0.633333             0.72 - 8.038818e-04           8.038818e-04
# 71    0.988235     0.933333  0.976         1    1.000000             0.96 - 1.016385e-03           1.016385e-03
# 362   0.994118     0.958333  0.923         1    0.966667             0.91 - 1.097642e-03           1.097642e-03
# 392   0.958824     0.933333  0.912         1    0.766667             0.84       1.179867e-03           1.179867e-03
# 328   0.952941     0.933333  0.877         1    0.766667             0.80 - 1.184212e-03           1.184212e-03
# 113   0.941176     0.916667  0.856         0    0.633333             0.72       1.305672e-03           1.305672e-03
# 130   0.997059     0.950000  0.976         1    0.900000             0.96 - 1.369249e-03           1.369249e-03
# 144   0.955882     0.933333  0.896         1    0.566667             0.80 - 1.485085e-03           1.485085e-03
# 32    0.994118     0.983333  0.940         1    0.766667             0.91       1.639760e-03           1.639760e-03
# 439   0.917647     0.875000  0.846         0    0.433333             0.66 - 1.780885e-03           1.780885e-03
# 53    0.952941     0.933333  0.810         1    0.700000             0.72 - 1.918663e-03           1.918663e-03
# 201   0.926471     0.916667  0.846         1    0.566667             0.72 - 1.942066e-03           1.942066e-03
# 462   0.902941     0.875000  0.794         0    0.666667             0.62 - 2.026021e-03           2.026021e-03

# SUGGESTED IMPROVEMENTS
#   ANALYZE DATA WITH COMBINED ATTRS (see if it shouldn't be combined)
# AFTER ANALYZING MISSING ATTRS
#   TRY HARMONIC MEAN OF SCORES
#   DROP RESEARCH EXP